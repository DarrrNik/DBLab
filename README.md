## Описание библиотек ##
### SQLite ###
SQLite - библиотека, написанная на языке С и работающая с файлами ``` .db ```, т.е. локально, без сервера. Главный её плюс: она - кроссплатформенна, программы с ней могут работать и на мобильных телефонах. 

Реализация запросов:
1. Импорт библиотеки
2. Подключение к файлу
3. Создание курсора
4. Выполнение запросов через курсор
5. Закрыть курсор и коннект в обратном от создания порядке

```python
import sqlite3
conn = sqlite3.connect(dbPath)
cursor = conn.cursor()
##############################
cursor.execute(sqlQueries[i])
##############################
cursor.close()
conn.close()
```
### DuckDB ###
DuckDB - самая быстрая библиотека из тестированных, для работы использует файлы `.db`. Библиотека создана, чтобы работать с большими базами данных с одним потоком. Её не стоит использовать на серверах с множеством обращений за раз.

Реализация запросов:
  1. Импорт библиотеки
  1. Скачать расширение (опционально)
  2. Подключение к файлу `.db`
  3. Создание курсора на основе подключения
  4. Через курсор выполнить нужные запросы к базе данных
  5. Закрыть курсор и конект в обратном от создания порядке
``` python
import duckdb
duckdb.install_extension("sqlite")
conn = duckdb.connect(dbPath)
cursor = conn.cursor()
##################################
cursor.execute(queries[j])
##################################
cursor.close()
conn.close()
```

### psycopg2 ###
Очередь за psycopg2. Библиотека работает с сервером и является наиболее популярной для использования с PostgreSQL базами данных. Позволяет работать с базами многопоточно (несколько процессов (клиентов) могут обращаться к базе одновременно).

Реализация запросов:
  1. Импорт библиотеки
  2. Подключение к серверу
  3. Создание курсора
  4. Выполнение запросов
  5. Закрытие курсора и коннекта
``` python
import psycopg2
conn = psycopg2.connect(
            host = host, 
            user = user, 
            password = password, 
            database = database
            )

cursor = conn.cursor()
################################
cursor.execute(queries[i])
################################
cursor.close()
conn.close()
```
### Pandas ###
Pandas -  оптимизированная библиотека, написанная с использованием языков Cython и C в "критичных местах". Библиотека стримится стать наиболее используемой и "сильной" в `Data Analysis` и `Data Mining`. `DataFrame` является основным объектом для работы с данными.

Реализация запросов:
1. Импорт нужных библиотек и функций
2. Создание движка на основе сервера
3. Выполнение запросов
4. Закрытие движка
``` python
import pandas
from sqlalchemy import create_engine
engine = create_engine(server)
#############################
pandas.read_sql(queries[i], con = engine)
#############################
engine.dispose()
```

### SQLAlchemy ###
SQLAlchemy - библиотека, написанная на `python`. Ее особенность Core или ORM. Она позволяет работать с ООП (классами) и выполнять запросы не зависимо от SQL - многие SQL запросы реализованы через функции. Запросы можно производить на основе движка как через сессию, так и через коннект.

Реализация запросов:
1. Подключение библиотеки
2. Создание движка на основе сервера
3. Подключение к серверу через движок
4. Выполнение запросов
5. Закрытие коннекта и движка
``` python
import sqlalchemy
engine = sqlalchemy.create_engine(server, echo = False)
conn = engine.connect()
###########################
conn.execute(sqlalchemy.text(queries[j]))
###########################
conn.close()
engine.dispose()
```

### Инструкция к запуску ###

Прежде всего, нужно клонировать данный репозиторий git к себе на машину. В файле "config.py" поменять значения переменных `host` (имя хоста в pgadmin), `user` (имя пользователя в pgadmin), `password` (пароль от пользователя в pgadmin), `database` (название базы данных в pgadmin), `port` (порт в pgadmin), `tableName` (имя таблицы в базах данных), `dbPath` (путь к файлу `.db`. Если файл не существует, то он создастся, если существует, то останется таким же), `bigCsvPath` (путь к большому файлу с расширением `.csv`), `tinyCsvPath` (путь к маленькому файлу с расширением `.csv`). Для создания файла `.db` используется большой файл, для создания таблицы в pgadmin (PostgreSQL) используется маленький файл. При запуске программы база данных в pgadmin (PostgreSQL) уже должна существовать.
Выбор запуска библиотек для теста происходит через переменные `t_DuckDB`, `t_Pandas`, `t_psycopg2`, `t_SQLAlchemy`, `t_SQLite`, изменяя их значение на `True` (запускать) или `False` (не запускать).
Через интерпретатор запускать файл `main.py`, используя версию python 3.10.0.

## Впечатления от библиотек ##
### SQLite ###

Синтаксис практически не отличается от некоторых других библиотек таких как psycopg2 и DuckDB. При работе с библиотекой возникла проблема в создании запроса, а именно: библиотека не понимала функции `EXTRACT`, из-за чего в запросах 3 и 4 пришлось заменить данную функцию на `STRFTIME`. Запросы в других библиотеках:
``` SQL
SELECT "passenger_count", EXTRACT(year FROM "tpep_pickup_datetime"), COUNT(*)
    FROM "DataBase" GROUP BY 1, 2;
```
``` SQL 
SELECT "passenger_count", EXTRACT(year FROM "tpep_pickup_datetime"), ROUND("trip_distance"), COUNT(*)
    FROM "DataBase" GROUP BY 1, 2, 3 ORDER BY 2, 4 DESC;
```
Запросы в `sqlite3`:
``` SQL
SELECT "passenger_count", STRFTIME('%Y', "tpep_pickup_datetime"), COUNT(*)
    FROM "DataBase" GROUP BY 1, 2;
```
``` SQL
SELECT "passenger_count", STRFTIME('%Y', "tpep_pickup_datetime"), ROUND("trip_distance"), COUNT(*)
    FROM "DataBase" GROUP BY 1, 2, 3 ORDER BY 2, 4 DESC;
```
### DuckDB ###

При ее использовании возникло несколько трудностей. Прежде всего, библиотека не устанавливалась падая с ошибкой на этапе загрузки. Эту проблему удалось решить, перейдя на версию `python` 3.10.0 с 3.12.0. После установки появилась новая проблема. Нужно было установить расширение "sqlite", ошибка в терминале подсказывала, что стоит попробовать написать ```INSTALL sqlite```, но куда именно было непонятно. С помощью интернета выяснилось, что нужно было создать запрос с этим текстом или вызвать команду ``` duckdb.install_extension("sqlite") ```. После выполнения данной функции все заработало. Было немного неприятно иметь с этим дело после работы в других библиотеках.

### psycopg2 ###

Работа с библиотекой не вызвала трудностей. При вводе в интернете "psycopg2" превой же ссылкой предлагается сайт с документацией данной библеотеки и примерами корректного использования. 10/10

### Pandas ###

Самая простая в использовании библиотека. Не возникло проблем при пользовании.

### SQLAlchemy ###

При использовании SQLAlchemy возникло несколько проблем. Сперва я столкнулась с реализацией запросов через функции библиотеки, а не код SQL. От нее пришлось отказаться, потому что не было классов и запросы все же нужно было вводить через код. После написания кода для выполнения запроса возникла проблема в виде перевода строки python в "текст" с помощью функции `sqlalchemy.text(queries[j])`. Эта функция используется для составления текстовой инструкции для передачи в запрос практически без изменений, что именно она делает со строкой и почему нельзя вставить обычную строку мне сайт с документацией не раскрыл.

## Сравнение времени работы библиотек ##
![Графики сравнения времени](https://github.com/DarrrNik/DBLab/assets/139496828/07c7869a-7420-416b-ae7c-a3925c0e2e92)

### SQLite ###
Производительность библиотеки в разы хуже, чем у других библиотек, что видно из привиденного графика. Это обусловлено в основном тем, что она работала на большой базе данных. Видимо ее предназначением является работа в основном с маленькими базами данных, в отличие от DUckDB.

### DuckDB ###

Она хорошо показала себя в данном бенчмарке, потому что она буквально была создана для работы с большими базами данных и одним клиентом. Скорее всего, при множественных запросах к одной базе от разных программ она показала бы себя хуже, об этом говорит сайт разработчика DuckDB. 

### psycopg2, Pandas и SQLAlchemy ###

Я решила обьединить данные библиотеки в один параграф, так как время выполнения запроса у них примерно одинаково (0.825 +- 0.003). Это обусловлено работой через PostgreSQL и маленьким размером базы данных. На больших данных они явно устпали бы DuckDb.
